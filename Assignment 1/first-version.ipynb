{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T14:02:15.322455Z","iopub.status.busy":"2024-10-16T14:02:15.321949Z","iopub.status.idle":"2024-10-16T14:02:15.346184Z","shell.execute_reply":"2024-10-16T14:02:15.345017Z","shell.execute_reply.started":"2024-10-16T14:02:15.322398Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/martinlebras/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import fasttext\n","import re\n","import collections\n","import string\n","from num2words import num2words\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import json\n","from tqdm import tqdm\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_file_path = \"data/test.csv\"\n","df_test = pd.read_csv(test_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","json_path = \"data/corpus.json\"\n","with open(json_path,\"r\",encoding='utf-8') as file:\n","    data = json.load(file)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def create_language_corpus(corpus: list) -> dict:\n","    language_corpus = {\"en\":[],\"fr\":[],\"es\":[],\"de\":[],\"it\":[],\"ar\":[],\"ko\":[]}\n","    for document in corpus:\n","        language_corpus[document[\"lang\"]].append(document)\n","    for language in tqdm(language_corpus.keys()):\n","        with open(f\"corpus_{language}.json\",\"w\",encoding='utf-8') as file:\n","            json.dump(language_corpus[language],file,ensure_ascii=False)\n","    del language_corpus"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def lowercase_sentence_start(text):\n","\n","    def lowercase_match(match):\n","        return match.group(1) + match.group(2).lower()\n","    \n","    pattern = r'([.!?]\\s+|^)([A-Z])'\n","    \n","    return re.sub(pattern, lowercase_match, text)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def convert_numbers_to_words(text, lang='en'):\n","    \n","    def replace_number(match):\n","        number_str = match.group(0)\n","        \n","        if '.' in number_str:\n","            return num2words(float(number_str), lang=lang)\n","        else:\n","            return num2words(int(number_str), lang=lang)\n","        \n","    pattern = r'\\b\\d+(\\.\\d+)?\\b'\n","    \n","    # Substitute all matched numbers with their word equivalents\n","    return re.sub(pattern, replace_number, text)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def remove_stopwords(text, lang='english'):\n","\n","    try:\n","        stop_words = set(stopwords.words(lang))\n","    except:\n","        stop_words = set()\n","    word_tokens = word_tokenize(text)\n","    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n","    \n","    return filtered_text"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T17:20:51.059562Z","iopub.status.busy":"2024-10-15T17:20:51.059024Z","iopub.status.idle":"2024-10-15T17:20:51.066148Z","shell.execute_reply":"2024-10-15T17:20:51.064744Z","shell.execute_reply.started":"2024-10-15T17:20:51.059515Z"},"trusted":true},"outputs":[],"source":["def preprocess_document(text: str, lower: bool, stopwords: bool, number: bool, language: str): #lower, remove punctuation and stopwords in the given language, convert numbers to their text value, handle names with captial letters\n","    language_mapping = {\"en\":\"english\",\"fr\":\"french\",\"de\":\"german\",\"es\":\"spanish\",\"ar\":\"arabic\",\"ko\":\"korean\",\"it\":\"italian\"}\n","    if lower:\n","        text = lowercase_sentence_start(text)\n","    else:\n","        text = text.lower()\n","    if number:\n","        text = convert_numbers_to_words(text, language)\n","    pattern = f\"[{re.escape(string.punctuation)}]\"\n","    text = re.sub(pattern, '', text)\n","    if stopwords:\n","        words = remove_stopwords(text,language_mapping[language])\n","    else:\n","        words = text.split(\" \")\n","    return words"]},{"cell_type":"markdown","metadata":{},"source":["Model embedding per language\n","Issue for query across languages \n","Multilingual model or consider most relevant documents are the one with same languages ? Check train data\n","Sentence embedding with TF-IDF in the document (?) and then regular aggregation across sentences of the document (min,max,mean ?)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def get_document_vocabulary(words: str) -> dict:\n","    return dict(collections.Counter(words))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def get_corpus_frequencies(corpus: list, language: str) -> dict:\n","    corpus_dict = {}\n","    for document in tqdm(corpus):\n","        corpus_dict[document[\"docid\"]] = get_document_vocabulary(preprocess_document(document[\"text\"],False,True,False,language))\n","    return corpus_dict"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def get_corpus_vocabulary(corpus_frequencies: dict) -> list:\n","    vocabulary = list()\n","    for doc_id in tqdm(corpus_frequencies.keys()):\n","        document_vocabulary = list(corpus_frequencies[doc_id].keys())\n","        vocabulary.extend(document_vocabulary)\n","    return list(set(vocabulary))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def term_frequency(word: str, document_vocabulary: dict) -> float:\n","    return document_vocabulary[word] / max(document_vocabulary.values())"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def inverse_document_frequency(corpus_frequencies: dict, corpus_vocabulary: list) -> dict:\n","\n","    document_count_per_word = collections.defaultdict(int)\n","    \n","    for doc_id, word_freq in corpus_frequencies.items():\n","        for word in word_freq.keys():\n","            document_count_per_word[word] += 1\n","    \n","    num_documents = len(corpus_frequencies)\n","    \n","    idf = {}\n","    for word in tqdm(corpus_vocabulary):\n","        n_word = document_count_per_word[word]\n","        idf[word] = float(np.log(num_documents / n_word))\n","    \n","    return idf"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T14:19:14.808489Z","iopub.status.busy":"2024-10-16T14:19:14.807341Z","iopub.status.idle":"2024-10-16T14:19:14.820206Z","shell.execute_reply":"2024-10-16T14:19:14.818441Z","shell.execute_reply.started":"2024-10-16T14:19:14.808429Z"},"trusted":true},"outputs":[],"source":["def tf_idf(word: str, document_vocabulary: dict, idf: dict) -> float:\n","    return term_frequency(word, document_vocabulary)*idf[word]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def tf_idf_mean_agg(model, document_vocabulary: dict, idf: dict) -> np.array: #Consider a weight normalization step if necessary\n","    first_word = True\n","    weights = 0\n","    for word in document_vocabulary.keys():\n","        word_embedding = model.get_word_vector(word)\n","        if first_word:\n","            document_embedding = np.zeros_like(word_embedding)\n","            first_word = False\n","        weight = tf_idf(word, document_vocabulary, idf)\n","        document_embedding += word_embedding*weight\n","        weights += weight\n","    document_embedding = document_embedding/weights\n","    return document_embedding\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def mean_agg(model, document_vocabulary: dict) -> np.array: #Consider a weight normalization step if necessary\n","    first_word = True\n","    count = 0\n","    for word in document_vocabulary.keys():\n","        word_embedding = model.get_word_vector(word)\n","        if first_word:\n","            document_embedding = np.zeros_like(word_embedding)\n","            first_word = False\n","        document_embedding += word_embedding\n","        count += 1\n","    document_embedding = document_embedding/count\n","    return document_embedding"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def embed_corpus(corpus: list, model, language: str, freq: bool) -> dict:\n","    documents_embeddings = {}\n","    if freq:\n","        corpus_frequencies = get_corpus_frequencies(corpus, language)\n","        corpus_vocabulary = get_corpus_vocabulary(corpus_frequencies)\n","        idf = inverse_document_frequency(corpus_frequencies, corpus_vocabulary)\n","        with open(f\"data/corpus_freq_{language}.json\",\"w\",encoding='utf-8') as freq_file:\n","            json.dump(corpus_frequencies, freq_file, ensure_ascii=False)\n","        with open(f\"data/corpus_vocab_{language}.json\",\"w\",encoding='utf-8') as vocab_file:\n","            json.dump(corpus_vocabulary, vocab_file, ensure_ascii=False)\n","        with open(f\"data/corpus_idf_{language}.json\",\"w\",encoding='utf-8') as idf_file:\n","            json.dump(idf, idf_file, ensure_ascii=False)\n","    else:\n","        with open(f\"data/corpus_idf_{language}.json\",\"r\",encoding='utf-8') as idf_file:\n","            idf = json.load(idf_file)\n","    print(f\"Embedding documents in {language}\")\n","    for document in tqdm(corpus):\n","        document_vocabulary = get_document_vocabulary(preprocess_document(document[\"text\"],False,True,False,language))\n","        documents_embeddings[document[\"docid\"]] = tf_idf_mean_agg(model, document_vocabulary, idf).tolist()\n","    print(f\"Documents embedded in {language}\")\n","    return documents_embeddings"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def cosine_similarity(word_a: np.array, word_b: np.array) -> float:\n","    sumxx, sumxy, sumyy = 0, 0, 0\n","    for i in range(len(word_a)):\n","        x = word_a[i]; y = word_b[i]\n","        sumxx += x*x\n","        sumyy += y*y\n","        sumxy += x*y\n","    if sumxy == 0:\n","        result = 0\n","    else:\n","        result =  sumxy / np.sqrt(sumxx*sumyy)\n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["Check for titles in the documents and if there is one, handle it differently\n","\n","Create a separate corpus for each language in order to have a separate vocabulary for each language"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading corpus in en\n","Corpus loaded in en\n","Loading model in en\n","Model loaded in en\n","Embedding corpus in en\n","Embedding documents in en\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 207363/207363 [43:31<00:00, 79.39it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Documents embedded in en\n","Corpus embedded in en\n","Saving embedded corpus in en\n","Embedded corpus saved in en\n","Loading corpus in fr\n","Corpus loaded in fr\n","Loading model in fr\n","Model loaded in fr\n","Embedding corpus in fr\n","Embedding documents in fr\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10676/10676 [07:06<00:00, 25.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Documents embedded in fr\n","Corpus embedded in fr\n","Saving embedded corpus in fr\n","Embedded corpus saved in fr\n","Loading corpus in it\n","Corpus loaded in it\n","Loading model in it\n","Model loaded in it\n","Embedding corpus in it\n","Embedding documents in it\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 11250/11250 [07:59<00:00, 23.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Documents embedded in it\n","Corpus embedded in it\n","Saving embedded corpus in it\n","Embedded corpus saved in it\n","Loading corpus in es\n","Corpus loaded in es\n","Loading model in es\n","Model loaded in es\n","Embedding corpus in es\n","Embedding documents in es\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 11019/11019 [07:16<00:00, 25.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Documents embedded in es\n","Corpus embedded in es\n","Saving embedded corpus in es\n","Embedded corpus saved in es\n","Loading corpus in de\n","Corpus loaded in de\n","Loading model in de\n","Model loaded in de\n","Embedding corpus in de\n","Embedding documents in de\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10992/10992 [06:53<00:00, 26.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Documents embedded in de\n","Corpus embedded in de\n","Saving embedded corpus in de\n","Embedded corpus saved in de\n","Loading corpus in ar\n","Corpus loaded in ar\n","Loading model in ar\n","Model loaded in ar\n","Embedding corpus in ar\n","Embedding documents in ar\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 8829/8829 [07:48<00:00, 18.85it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Documents embedded in ar\n","Corpus embedded in ar\n","Saving embedded corpus in ar\n","Embedded corpus saved in ar\n","Loading corpus in ko\n","Corpus loaded in ko\n","Loading model in ko\n","Model loaded in ko\n","Embedding corpus in ko\n","Embedding documents in ko\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7893/7893 [05:34<00:00, 23.59it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Documents embedded in ko\n","Corpus embedded in ko\n","Saving embedded corpus in ko\n","Embedded corpus saved in ko\n"]}],"source":["#print(\"Creating corpus for each language\")\n","#create_language_corpus(data)\n","#print(\"Corpus created for each language\")\n","#gc.collect()\n","languages = [\"en\",\"fr\",\"it\",\"es\",\"de\",\"ar\",\"ko\"]\n","for language in languages:\n","    print(f\"Loading corpus in {language}\")\n","    with open(f\"data/corpus_{language}.json\",\"r\",encoding='utf-8') as corpus_file:\n","        corpus = json.load(corpus_file)\n","    print(f\"Corpus loaded in {language}\")\n","\n","    print(f\"Loading model in {language}\")\n","    model = fasttext.load_model(f\"cc.{language}.300.bin\")\n","    print(f\"Model loaded in {language}\")\n","\n","    print(f\"Embedding corpus in {language}\")\n","    embedded_corpus = embed_corpus(corpus, model, language, False)\n","    print(f\"Corpus embedded in {language}\")\n","\n","    print(f\"Saving embedded corpus in {language}\")\n","    with open(f\"data/embedded_corpus_{language}.json\",\"w\",encoding='utf-8') as file:\n","        json.dump(embedded_corpus, file, ensure_ascii=False)\n","    print(f\"Embedded corpus saved in {language}\")\n","    del corpus, model, embedded_corpus\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["train_file_path = \"data/train.csv\"\n","df_train = pd.read_csv(train_file_path)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>query_id</th>\n","      <th>query</th>\n","      <th>positive_docs</th>\n","      <th>negative_docs</th>\n","      <th>lang</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>q-en-425512</td>\n","      <td>What is the connection between AAA and Lucha U...</td>\n","      <td>doc-en-798457</td>\n","      <td>['doc-en-810925', 'doc-en-634020', 'doc-en-143...</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>q-en-16636</td>\n","      <td>What is the medical use of iloperidone?</td>\n","      <td>doc-en-121692</td>\n","      <td>['doc-en-177976', 'doc-en-700330', 'doc-en-567...</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>q-en-282671</td>\n","      <td>Who was the provisional administrator in 1940?</td>\n","      <td>doc-en-750259</td>\n","      <td>['doc-en-805362', 'doc-en-413387', 'doc-en-827...</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>q-en-216614</td>\n","      <td>What was the critical reception of the film se...</td>\n","      <td>doc-en-703883</td>\n","      <td>['doc-en-685958', 'doc-en-84060', 'doc-en-2046...</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>q-en-156120</td>\n","      <td>What was the main Spanish record of the year i...</td>\n","      <td>doc-en-648393</td>\n","      <td>['doc-en-4307', 'doc-en-761696', 'doc-en-79426...</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>21870</th>\n","      <td>q-ar-1187</td>\n","      <td>احتفالية تلعب دورًا كبيرًا في تعزيز الترابط ال...</td>\n","      <td>doc-ar-8463</td>\n","      <td>['doc-ar-5304', 'doc-ar-1977', 'doc-ar-5843', ...</td>\n","      <td>ar</td>\n","    </tr>\n","    <tr>\n","      <th>21871</th>\n","      <td>q-ar-1188</td>\n","      <td>ما هو عدد أتباع كنيسة الأدفنتست في جزيرة سان ا...</td>\n","      <td>doc-ar-8469</td>\n","      <td>['doc-ar-6798', 'doc-ar-1489', 'doc-ar-3100', ...</td>\n","      <td>ar</td>\n","    </tr>\n","    <tr>\n","      <th>21872</th>\n","      <td>q-ar-1189</td>\n","      <td>من هو أنتاناس سمتا؟</td>\n","      <td>doc-ar-8476</td>\n","      <td>['doc-ar-2898', 'doc-ar-6787', 'doc-ar-3235', ...</td>\n","      <td>ar</td>\n","    </tr>\n","    <tr>\n","      <th>21873</th>\n","      <td>q-ar-1191</td>\n","      <td>سؤالي هو: ما هي الميزة التي كانت للإيرلنديين ف...</td>\n","      <td>doc-ar-8491</td>\n","      <td>['doc-ar-786', 'doc-ar-8084', 'doc-ar-3208', '...</td>\n","      <td>ar</td>\n","    </tr>\n","    <tr>\n","      <th>21874</th>\n","      <td>q-ar-1193</td>\n","      <td>من استحوذ على مجمع ساسكاتشوان للقمح عام 2007؟</td>\n","      <td>doc-ar-8506</td>\n","      <td>['doc-ar-2044', 'doc-ar-2480', 'doc-ar-115', '...</td>\n","      <td>ar</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>21875 rows × 5 columns</p>\n","</div>"],"text/plain":["          query_id                                              query  \\\n","0      q-en-425512  What is the connection between AAA and Lucha U...   \n","1       q-en-16636            What is the medical use of iloperidone?   \n","2      q-en-282671     Who was the provisional administrator in 1940?   \n","3      q-en-216614  What was the critical reception of the film se...   \n","4      q-en-156120  What was the main Spanish record of the year i...   \n","...            ...                                                ...   \n","21870    q-ar-1187  احتفالية تلعب دورًا كبيرًا في تعزيز الترابط ال...   \n","21871    q-ar-1188  ما هو عدد أتباع كنيسة الأدفنتست في جزيرة سان ا...   \n","21872    q-ar-1189                                من هو أنتاناس سمتا؟   \n","21873    q-ar-1191  سؤالي هو: ما هي الميزة التي كانت للإيرلنديين ف...   \n","21874    q-ar-1193      من استحوذ على مجمع ساسكاتشوان للقمح عام 2007؟   \n","\n","       positive_docs                                      negative_docs lang  \n","0      doc-en-798457  ['doc-en-810925', 'doc-en-634020', 'doc-en-143...   en  \n","1      doc-en-121692  ['doc-en-177976', 'doc-en-700330', 'doc-en-567...   en  \n","2      doc-en-750259  ['doc-en-805362', 'doc-en-413387', 'doc-en-827...   en  \n","3      doc-en-703883  ['doc-en-685958', 'doc-en-84060', 'doc-en-2046...   en  \n","4      doc-en-648393  ['doc-en-4307', 'doc-en-761696', 'doc-en-79426...   en  \n","...              ...                                                ...  ...  \n","21870    doc-ar-8463  ['doc-ar-5304', 'doc-ar-1977', 'doc-ar-5843', ...   ar  \n","21871    doc-ar-8469  ['doc-ar-6798', 'doc-ar-1489', 'doc-ar-3100', ...   ar  \n","21872    doc-ar-8476  ['doc-ar-2898', 'doc-ar-6787', 'doc-ar-3235', ...   ar  \n","21873    doc-ar-8491  ['doc-ar-786', 'doc-ar-8084', 'doc-ar-3208', '...   ar  \n","21874    doc-ar-8506  ['doc-ar-2044', 'doc-ar-2480', 'doc-ar-115', '...   ar  \n","\n","[21875 rows x 5 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["df_train"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["languages = [\"en\",\"fr\",\"es\",\"it\",\"de\",\"ar\",\"ko\"]\n","corpus_query = {language:df_train[df_train[\"lang\"] == language][[\"query_id\",\"query\"]].set_index(\"query_id\").to_dict()[\"query\"] for language in languages}"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def embed_queries(corpus: dict, model, language: str, freq: bool) -> dict:\n","    queries_embeddings = {}\n","    with open(f\"data/corpus_idf_{language}.json\",\"r\",encoding='utf-8') as idf_file:\n","        idf = json.load(idf_file)\n","    print(f\"Embedding queries in {language}\")\n","    for query_id, query_text in tqdm(corpus.items()):\n","        query_vocabulary = get_document_vocabulary(preprocess_document(query_text,False,True,False,language))\n","        try:\n","            queries_embeddings[query_id] = mean_agg(model, query_vocabulary).tolist()\n","        except:\n","            print(query_id)\n","            print(query_text)\n","    print(f\"Documents queries in {language}\")\n","    return queries_embeddings"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading corpus in en\n","Corpus loaded in en\n","Loading model in en\n","Model loaded in en\n","Embedding corpus in en\n","Embedding queries in en\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10000/10000 [00:01<00:00, 7965.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Documents queries in en\n","Corpus embedded in en\n","Saving embedded corpus in en\n","Embedded corpus saved in en\n","Loading corpus in fr\n","Corpus loaded in fr\n","Loading model in fr\n","Model loaded in fr\n","Embedding corpus in fr\n","Embedding queries in fr\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1608/1608 [00:00<00:00, 5724.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Documents queries in fr\n","Corpus embedded in fr\n","Saving embedded corpus in fr\n","Embedded corpus saved in fr\n","Loading corpus in it\n","Corpus loaded in it\n","Loading model in it\n","Model loaded in it\n","Embedding corpus in it\n","Embedding queries in it\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2151/2151 [00:00<00:00, 5826.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["q-it-867\n","chi?\n","Documents queries in it\n","Corpus embedded in it\n","Saving embedded corpus in it\n","Embedded corpus saved in it\n","Loading corpus in es\n","Corpus loaded in es\n","Loading model in es\n","Model loaded in es\n","Embedding corpus in es\n","Embedding queries in es\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2254/2254 [00:00<00:00, 5189.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Documents queries in es\n","Corpus embedded in es\n","Saving embedded corpus in es\n","Embedded corpus saved in es\n","Loading corpus in de\n","Corpus loaded in de\n","Loading model in de\n","Model loaded in de\n","Embedding corpus in de\n","Embedding queries in de\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1847/1847 [00:00<00:00, 5369.70it/s]"]},{"name":"stdout","output_type":"stream","text":["q-de-484\n","sein könnte. \n","Documents queries in de\n","Corpus embedded in de\n","Saving embedded corpus in de\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Embedded corpus saved in de\n","Loading corpus in ar\n","Corpus loaded in ar\n","Loading model in ar\n","Model loaded in ar\n","Embedding corpus in ar\n","Embedding queries in ar\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1817/1817 [00:00<00:00, 4433.72it/s]"]},{"name":"stdout","output_type":"stream","text":["q-ar-414\n","أيلول.\n","Documents queries in ar\n","Corpus embedded in ar\n","Saving embedded corpus in ar\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Embedded corpus saved in ar\n","Loading corpus in ko\n","Corpus loaded in ko\n","Loading model in ko\n","Model loaded in ko\n","Embedding corpus in ko\n","Embedding queries in ko\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2198/2198 [00:00<00:00, 7429.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Documents queries in ko\n","Corpus embedded in ko\n","Saving embedded corpus in ko\n","Embedded corpus saved in ko\n"]}],"source":["languages = [\"en\",\"fr\",\"it\",\"es\",\"de\",\"ar\",\"ko\"]\n","for language in languages:\n","    print(f\"Loading corpus in {language}\")\n","    corpus = corpus_query[language]\n","    print(f\"Corpus loaded in {language}\")\n","\n","    print(f\"Loading model in {language}\")\n","    model = fasttext.load_model(f\"cc.{language}.300.bin\")\n","    print(f\"Model loaded in {language}\")\n","\n","    print(f\"Embedding corpus in {language}\")\n","    embedded_queries = embed_queries(corpus, model, language, False)\n","    print(f\"Corpus embedded in {language}\")\n","\n","    print(f\"Saving embedded corpus in {language}\")\n","    with open(f\"data/embedded_queries_{language}.json\",\"w\",encoding='utf-8') as file:\n","        json.dump(embedded_queries, file, ensure_ascii=False)\n","    print(f\"Embedded corpus saved in {language}\")\n","    del corpus, model, embedded_queries\n","    gc.collect()"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def find_top_k_doc(queries,documents,k,language):\n","    query_matrix = list()\n","    queries_ids = list()\n","    for query_id, query_embeddings in queries.items():\n","        query_matrix.append(query_embeddings)\n","        queries_ids.append(query_id)\n","    query_matrix = np.array(query_matrix)\n","    queries_norms = np.diagonal(query_matrix.dot(query_matrix.T))\n","    queries_inverse_norms = np.linalg.inv(np.diag(np.sqrt(queries_norms)))\n","    \n","    doc_matrix = list()\n","    doc_ids = list()\n","    for doc_id, doc_embeddings in documents.items():\n","        doc_matrix.append(doc_embeddings)\n","        doc_ids.append(doc_id)\n","\n","    if language != \"en\":\n","        doc_matrix = np.array(doc_matrix)\n","        documents_norms = np.diagonal(doc_matrix.dot(doc_matrix.T))\n","        documents_inverse_norms = np.linalg.inv(np.diag(np.sqrt(documents_norms)))\n","        cosine_similarities = np.dot(queries_inverse_norms,np.dot(np.dot(query_matrix,doc_matrix.T),documents_inverse_norms))\n","    else:\n","        step = len(doc_matrix)//10\n","        cosine_similarities = np.zeros((len(queries_ids),len(doc_ids)))\n","        for i in range(10):\n","            if i != 9:\n","                doc_sub_matrix = np.array(doc_matrix[i*step:(i+1)*step])\n","            else:\n","                doc_sub_matrix = np.array(doc_matrix[i*step:])\n","            documents_norms = np.diagonal(doc_sub_matrix.dot(doc_sub_matrix.T))\n","            documents_inverse_norms = np.linalg.inv(np.diag(np.sqrt(documents_norms)))\n","            sub_cosine_similarities = np.dot(queries_inverse_norms,np.dot(np.dot(query_matrix,doc_sub_matrix.T),documents_inverse_norms))\n","            if i != 9:\n","                cosine_similarities[:,i*step:(i+1)*step] = sub_cosine_similarities\n","            else:\n","                cosine_similarities[:,i*step:] = sub_cosine_similarities\n","    top_k_per_query = cosine_similarities.argsort(axis=1)[::-1][:,:k]\n","    doc_ids = np.array(doc_ids)\n","    top_k_documents_id = dict()\n","    for i in range(len(queries_ids)):\n","        top_k_documents_id[queries_ids[i]] = doc_ids[top_k_per_query[i]].tolist()\n","    return top_k_documents_id"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["languages = [\"en\",\"fr\",\"es\",\"de\",\"ar\",\"ko\",\"it\"]\n","for language in languages:\n","    with open(f\"data/embedded_corpus_{language}.json\",\"r\",encoding='utf-8') as doc_file:\n","        corpus = json.load(doc_file)\n","    with open(f\"data/embedded_queries_{language}.json\",\"r\",encoding='utf-8') as query_file:\n","        queries = json.load(query_file)\n","    retrieved_documents = find_top_k_doc(queries,corpus,10,language)\n","    with open(f\"data/retrieved_doc_{language}.json\",\"w\",encoding='utf-8') as output_file:\n","        json.dump(retrieved_documents,output_file,ensure_ascii=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["languages = [\"en\",\"fr\",\"es\",\"de\",\"ar\",\"ko\",\"it\"]\n","for language in languages:\n","    with open(f'data/retrieved_doc_{language}.json', 'r') as f:\n","        retrieved_docs = json.load(f)\n","\n","    # Load ground truth from CSV file\n","    ground_truth = pd.read_csv('data/train.csv')\n","    ground_truth = ground_truth[ground_truth[\"lang\"] == language]\n","\n","    # Step 2: Initialize counters for evaluation\n","    total_positive = 0\n","    retrieved_positive = 0\n","    total_negative = 0\n","    retrieved_negative = 0\n","\n","    # Step 3: Evaluate each query\n","    for index, row in ground_truth.iterrows():\n","        query_id = row['query_id']\n","        positive_docs = {row['positive_docs']} # Convert list in string format to actual list\n","        negative_docs = set(eval(row['negative_docs'])) # Same for negative docs\n","        \n","        retrieved = set(retrieved_docs.get(str(query_id), []))  # Get the retrieved docs for this query\n","        \n","        # Count positives and negatives\n","        total_positive += len(positive_docs)\n","        total_negative += len(negative_docs)\n","        \n","        retrieved_positive += len(positive_docs.intersection(retrieved))  # How many positive docs were retrieved\n","        retrieved_negative += len(negative_docs.intersection(retrieved))  # How many negative docs were retrieved\n","\n","    # Step 4: Calculate percentages\n","    percentage_positive_retrieved = (retrieved_positive / total_positive) * 100 if total_positive > 0 else 0\n","    percentage_negative_retrieved = (retrieved_negative / total_negative) * 100 if total_negative > 0 else 0\n","\n","    # Step 5: Output results\n","    print(f\"Percentage of positive documents retrieved in {language}: {percentage_positive_retrieved:.2f}%\")\n","    print(f\"Percentage of negative documents retrieved in {language}: {percentage_negative_retrieved:.2f}%\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":9635715,"sourceId":85316,"sourceType":"competition"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
